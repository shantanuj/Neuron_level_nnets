{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some dependencies for making a neural network \n",
    "#from sklearn.datasets import california_housing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some preliminaries and descriptions (Make sure to read this)\n",
    "\n",
    "1. Goal of this particular neural network: Extract patterns from given observation inspired by human brain functioning. Note that inspiration does not equal representation, ie, neural network is not an exact replication of cognitive processing. Also a neural network may form a strategy to form robust representations for AI. It is by itself a machine learning technique. However, I have presented the below thoughts in context of AI. \n",
    "\n",
    "\n",
    "2. Given observations are currently represented as vectors of information (matrices, vectors seem the most convenient and intuitive way). \n",
    "    1. In a 'supervised setting' each/some observation has a corresponding outcome/label. \n",
    "    2. Personally I am intrigued by the process of feeding input data to an AI system? Should it be represented as streams of vectors (a matrix). What mechanism does the human brain use to feed in data through sensory agents? \n",
    "        1. Humans and living beings have two, and in my opinion, crucial mechanisms to obtain 'input data'- sensory agents and interactive agents. <b> Q: How do interaction and sensing play crucial roles in influencing cognitive processes?</b> \n",
    "        2. The environment surrounding the interaction/sense may also influence the cognitive process/output. The result too could be stored for recall later (memory). I am not aware of the exact mechanism of how memory is recalled or stored. <b> Again, this is an interesting area to explore. Should inputs be represented as a tuple of (input, environment, sensor_type, memory/recall, dependency on other inputs), or perhaps memory should be decoupled and part of processing unit as a current_state (hint of an FSM)? </b>\n",
    "        3. A particular interesting example of how environment and memory influence cognitive processing is 'priming.' Semantic priming influences how one may process a stream of words differently depending on previous exposure (memory) or the current environment. Thus for text tasks, I feel representing and including information on the above would be beneficial. <b>Now, taking this example forward, how can we form a 'representation' given some data? This is what neural networks/ deep learning may be used to do. Different architectures can be postulated to form more accurate/useful representations. From what I've experienced, forming more robust architectures for particular domains is quite popular. However, this may lead to domain specific architectures, and perhaps all may be collaborated to form an AI system.<i>I like working on forming architectures within an AI framework (see primary points below)</i></b>\n",
    "        \n",
    "    3. Depending on task, we may represent input data (observations) differently:\n",
    "        1. Example: Images: Represent them as pixel intensities\n",
    "        2. Example 2: Documents: Global sense: Word vectors- represent each word by context features\n",
    "        3. Example 3: ....\n",
    "\n",
    "3. <b>A concise (not exhaustive) summary/listing of primary components of human brain processing</b> that I found useful (will add proper academic papers and verified theory later) : http://www.teach-nology.com/teachers/methods/info_processing/ Key points:\n",
    "    1. Input through different stimuli and encoded storage in different modes- structural, phonemic, semantic \n",
    "    2. Transformation algorithms- bottom up processing and top down processing. THIS Is what we'll explore with neural networks. \n",
    "    3. Attention filter for signals and selection of relevant cognitive processes\n",
    "    4. Short term memory: Electric signal loop through certain neurons; Long term memory: Protein structures\n",
    "    5. Organization of knowledge in brain: Many postulated models\n",
    "    6. Retrieval of memory/recall: retrieval cue, priming, distortions. <b>How are words formed for conveying a particular thought? </b>\n",
    "    7. <b>How is emotion implemented without language, just senses?</b> \n",
    "    \n",
    "   <b>Chollet: The ability to convey intent and emotions through voice seems like a universal constant that predates language. Ever noticed how the tone you'd use to ask a question, or give an order, is the same in essentially every language?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purpose of this notebook\n",
    "\n",
    "This notebook explores part 2B of the above: How to create transformation algorithms that transform inputs into more useful representations for given objectives. \n",
    "\n",
    "Additionally, it provides a foundation/accustoms one to effectively adapt new biological/mathematical concepts into code. \n",
    "\n",
    "The notebook starts with a simple neural network model- Artificial Neural Network- and then explores variants of the same. It also covers necessary mathematical and comp sci concepts in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data- assume a matrix. Each row represents a particular vector which represents a set of observations.\n",
    "#Assuming we process parallely and independently, we can process multiple input events together. This, we call a batch of inputs which will yield a batch of outputs. We will discuss non-independent batches later. \n",
    "\n",
    "x= np.array([[1,2,3],[4,5,6]])\n",
    "y= np.array([[1,10],[2,4]]) # we want two ouputs for each set of observations\n",
    "#Feel free to replace the above x_in and y with any dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our standard neural network aims to do the following (bold parts are crucial): \n",
    "\n",
    "1. Given set of inputs and outputs, find the best transformation of inputs (possibly and usually, multiple transformations) to obtain outputs. This it does through 'weighting each previous input then applying a non-linearity to the weighted input (other mechanisms- perhaps biological inspired activations, not necessarily 'non-linear', can be used)\n",
    "\n",
    "2. In finding the best transformations, we are finding how to weight certain data. Given a lot of data, we are eventually extracting recurring patterns in the data. \n",
    "\n",
    "3. The best transformations are dependent on weights for each neuron (other variables can be included, but proper mechanisms need to be developed for how to modify the variables for best usage. Ex: backprop/gradient passing is used for finding best weights. However, it is disputed whether this is the best mechanism to adjust weights.)\n",
    "\n",
    "4. In this particular ANN by using backprop/gradient passing we will try to find (not guaranteed to be optimum-mathematically or biologically) the best weights/variables to satisfy a given objective/cost function\n",
    "\n",
    "5. The cost function plays the role of a metric to see whether our model is outputting satisfactory values (DO \n",
    "HUMANS have cost functions?)\n",
    "\n",
    "6. <b>New variabes can be added in this paradigm- and to adjust them, we need to derive and implement the gradient with respect to the cost function. </b>\n",
    "> Ex: An example variable can be associated primes to a particular input.  \n",
    "\n",
    "<b>Otherwise new activation/feedforward computation mechanisms, variable-adjusting mechanisms and objective functions- specific to the variable- need to be proposed and implemented. </b> \n",
    "\n",
    ">Ex: In an hebbian learner the variable is co-occuring neurons, objective is unknown, but mechanism is to increase weights/strength of connection whenever there is co-occurance.\n",
    "\n",
    "7. <b>For every new variable, think of three things- \n",
    "    1. How does it impact forward value calculation.\n",
    "    2. How should we adjust the variable (if needed) in light of correctness/objective function\n",
    "    3. What objective function (if any) needs to be used</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1. AI/nnet\n",
    "    1. Framework for creating nnet models. Variables (forward_calc_mechanism- see RNN, adjust_mechansim, objective_func)\n",
    "    3. Environmental and sensory input form impact on learning.\n",
    "    4. Experimental learning.\n",
    "    5. Organization of knowledge in brain inspired networks.\n",
    "    6. Memory and concept retrieval based on environment. Retrieval, distortions, priming effects.\n",
    "    7. How are words formed for conveying a particular thought\n",
    "    8. How to validate biological models? \n",
    "\n",
    "    \n",
    "\n",
    "2. Brain focused/psychology\n",
    "    1. Does law of psychological low cost and dependence on system 1, help human processing? Observations and conclusions are formed through impressions of system 1 rather than system 2. Is this always detrimental and should it be included in AI nets. Is there no benefit for such a mechanism, or is there a hidden linkage? Same for priming.\n",
    "    2. Semantic priming and semantic networks\n",
    "    \n",
    "    \n",
    "3. Research focused:\n",
    "    1. Fake news: Break up into attaining relevant facts/questions postulated by an article. Get answers from parallel corpus of historical/new articles from good websites. What kind of fake news does this answer to- fact checking ones? Using common sense to come up with questions--wherever common sense does not exist, is it a question? <b>Deep learning can be used to compare postulated question representations with actual question representations.</b>\n",
    "    2. ABSA domain translation: When do structural correspondences exist in joint space?\n",
    "    3. Is reading Perceptrons worth it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a standard neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below focuses on creating a neural network from a neuron level. The focus is not entirely on matrix transformations and layer wise transformations, but also on how particular neurons may act differently in the same computation step.\n",
    "### Hence, this is not meant to be a scalable/efficient network, but hopefully a playground to create slightly different neural network architectures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[4,3,2],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-241b4f2d7ff3>, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-241b4f2d7ff3>\"\u001b[0;36m, line \u001b[0;32m103\u001b[0m\n\u001b[0;31m    return softmaxed_out,  c           c\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "np.random.seed(5)\n",
    "##Auxiliary functions \n",
    "\n",
    "def visual_plot(x,y):\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    \n",
    "def weighted_linear(x,W=1,der= False, visual=False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), weighted_linear(np.arange(-10,10,0.1),1))\n",
    "    if(not der):\n",
    "        return x #np.dot(x,np.transpose(W)) #actually it should just be x since we're assuming weights are computed in prior step\n",
    "    else:\n",
    "        return x/x #this should not be 1, think why\n",
    "\n",
    "def linear(x, der = False): #incase of confusion\n",
    "    return weighted_linear(x, W=1, der=der)\n",
    "\n",
    "def sigmoid(z, der=False, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), sigmoid(np.arange(-10,10,0.1)))\n",
    "    if(der):\n",
    "        return der_sigmoid(z)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def der_sigmoid(z, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), del_sigmoid(np.arange(-10,10,0.1)))\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu(z,visual = False):\n",
    "    return np.array(map(lambda x: max(0,x)),z)\n",
    "    \n",
    "def tanh(z, der = False, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), tanh(np.arange(-10,10,0.1)))\n",
    "    if(der):\n",
    "        return der_tanh(z)\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "\n",
    "def der_tanh(z, visual = False):\n",
    "    return 1- np.power(tanh(z),2)\n",
    "\n",
    "'''\n",
    "def mse(correct_y, predicted_y, der = False, visual = False):\n",
    "    #calculates mse cost given predicted y vector and correct y vector for a single training example\n",
    "    \n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-5,10,0.1),np.power([5]-np.arange(-5,10,0.1),2)/2)\n",
    "    if(not der): #mse is calculated as 1/2n* (squared difference)\n",
    "        return np.mean(np.power(correct_y-predicted_y,2), axis=0)/(2) #average over rows\n",
    "        return np.power(correct_y-predicted_y, 2)/2\n",
    "    else:\n",
    "        #derivative with respect to predicted_y\n",
    "        return np.mean(predicted_y-correct_y, axis =0) # average\n",
    "        return predicted_y - correct_y\n",
    "    \n",
    "'''\n",
    "\n",
    "def l2_norm(x, vector_axis=1):\n",
    "    \"\"\"x is an input tensor\n",
    "    output is l2 norm of input_shape_with_last_axis=1\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(x,2), axis =vector_axis))\n",
    "    \n",
    "def mse(correct_y, predicted_y, der_and_avg= True, der = False, visual = False):\n",
    "    '''\n",
    "    mse(y,y') = (1/2m) * sum_across_examples(y-y')^2 where m is batch size/number of training examples \n",
    "    calculates mse cost given predicted y matrix and correct y' matrix for a given batch'''\n",
    "    '''If inputs are of shape m*n then output is of tuple (m*n, 1*n, m*n)'''\n",
    "    assert correct_y.shape == predicted_y.shape\n",
    "    \n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-5,10,0.1),np.power([5]-np.arange(-5,10,0.1),2)/2)\n",
    "    \n",
    "    diff = predicted_y - correct_y\n",
    "    m = predicted_y.shape[0]\n",
    "    \n",
    "    if(der_and_avg):\n",
    "        der_c_wrt_pred_y = diff/m ##m*n of derivatives \n",
    "        mse_individual = l2_norm(diff) #m*1 mse l2 norm for each example\n",
    "        mse_avg = np.mean(mse_individual, axis =0)/2 #1*1 mse avg according to formula\n",
    "        return mse_avg, der_c_wrt_pred_y \n",
    "    else:    \n",
    "        if(not der): #just the avg cost\n",
    "            mse_individual = l2_norm(diff)\n",
    "            return np.mean(mse_individual)/2 # mse_individual  #THIS IS 1*n (averaged across batch) \n",
    "        #return np.sum(np.power(correct_y-predicted_y,2))/(correct_y.shape[0]*2)\n",
    "        else:#just the cost derivate\n",
    "            return diff/m  #np.mean(diff, axis =0) #THIS IS m*n, derivative of mse with respect to prediction\n",
    "\n",
    "\n",
    "def softmax(z,der=False, visual=False):\n",
    "    '''Return a more uniform (gibbs) distribution over inputs'''\n",
    "    e_to_x = np.exp(z)\n",
    "    e_to_x_sum = np.sum(e_to_x, axis = 1)\n",
    "    softmaxed_out = e_to_x/e_to_x_sum\n",
    "    if(not der):\n",
    "        return softmaxed_out\n",
    "    else:\n",
    "        return softmaxed_out,  c           \n",
    "    \n",
    "def bitwise_cross_entropy(correct_y, predicted_y, der=False, visual= False):\n",
    "    '''correct_y is of shape m*n where n = ceil(lg(num_outputs))\n",
    "    Use this when neurons in output layer is bitwise (not one hot) encoded\n",
    "    '''\n",
    "    None\n",
    "    \n",
    "    \n",
    "def cross_entropy(correct_y, predicted_y, der=False, visual = False): #assume correct_y is a scalar for each training example\n",
    "    '''\n",
    "    correct_y is a one hot encoded array of shape m*n \n",
    "    predicted_y is vector of output probabilies(softmax)\n",
    "    if conditions are used in case a single dimensional vector/single point input is given\n",
    "    '''\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(0,1,0.01), -np.log(np.arange(0,1,0.01)))\n",
    "        #visual_plot(np.arange(-10,10,0.1))\n",
    "    \n",
    "    if(correct_y.ndim>1):\n",
    "        indices = np.argmax(correct_y,axis=1) #correct_y example [[[0,0,1,0],[0,0,0,1]]] given four classes and two training examples \n",
    "        predicted_prob_given_indices = map(lambda y,ind: y[ind], predicted_y, indices) #select prob values of predictions given the index\n",
    "        num_inputs = correct_y.shape[0]\n",
    "    else:\n",
    "        #stochastic example\n",
    "        indices = np.array(np.argmax(correct_y))\n",
    "        predicted_prob_given_indices = predicted_y[indices]\n",
    "        num_inputs = 1\n",
    "    return np.sum(-np.log(predicted_prob_given_indices))/num_inputs\n",
    "\n",
    "def hadamard_product(x,y): #element wise product\n",
    "    return np.multiply(x,y)\n",
    "\n",
    "activation_dict = {1:sigmoid, 2: tanh, 3: weighted_linear, 4: relu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.7]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0,0.2,0.8,0],[0,0.1,0.2,0.7]]\n",
    "ind = np.argmax([[0,0,1,0],[0,0,0,1]], axis=1)\n",
    "#predicted_prob_given_indices = map(lambda y,ind: y[ind],list(zip(y, ind)))\n",
    "map(lambda x,a: x[a], y,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0098660371654399892"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,0,1]])\n",
    "l2_norm(x,1)\n",
    "\n",
    "np.random.random(10) -np.random.random(10)\n",
    "#help(np.random.random)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember the purpose of this neural network is to transform data into a better representation in a 'bottom up' approach. \n",
    "#A transformation is simply a function of x: f(x). While there exist many different forms/models of transformation functions- linear weights, polynomials, any function one thinks will help\n",
    "\n",
    "#A neuron, however, is modelled based on its biological components. Here's a good summary: https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system/a/overview-of-neuron-structure-and-function. A typical neuron consists of the following:\n",
    "#1) Neurons form a network- a top down processing network leads to later neurons forming more abstract representations\n",
    "#2) Neurons have activation signals which influence other activation signals\n",
    "#3) A neuron might weight each 'synaptic input' differently\n",
    "#4) Each neuron represents a particular representation. \n",
    "#5) Remember in this architecture, each neuron represents a value between 0 and 1 (non-fuzzy) value. The first layer neurons correspond to the non-complex representations, which become more complex as they activate further neurons. \n",
    "#I have used an OOP approach since it would be interesting to model different type of neurons. However, for scalable implementations a bunch of layer wise matrices should be used.  \n",
    "#6) We will explore adding a new variable as well\n",
    "class neuron:\n",
    "    \n",
    "    def __init__(self, input_size,activation = sigmoid, lr= 1, belongs_to = None):\n",
    "        self.shape = (input_size,1)\n",
    "        self.weights = np.random.random(self.shape) - np.random.random(self.shape) #np.random.random(self.shape) #initialize random values  #np.zeros(self.shape)#\n",
    "        self.bias = np.array([0])\n",
    "        self.activation = activation\n",
    "        self.learning_rate = lr\n",
    "        self.surrounding_activations = None\n",
    "        self.belongs_to = belongs_to\n",
    "        self.neighbours = []\n",
    "        #self.previous_layer_activations = []\n",
    "        self.error_wrt_C = None\n",
    "        self.historical_activations = None #Hebbian learner to mantain co-occuring high activations together-> if n1 and n2 co-occur a lot, then if n1 occurs add a bias term?\n",
    "    def forward_pass(self,input_x, weighted_as_well = False): #weighted as well means just weighted, not activated\n",
    "        '''Input x is of shape m*n where m is batch size and n is input dims'''\n",
    "        weighted_input = np.dot(input_x, self.weights)+self.bias\n",
    "        if(weighted_as_well): #this returns both activated and non activated output \n",
    "            return (self.activation(weighted_input),weighted_input, self.activation(weighted_input, der = True)) \n",
    "        else:\n",
    "            return self.activation(weighted_input)\n",
    "    def adjust_parameters(self):\n",
    "        None\n",
    "    def calc_error(self,correct_y, predicted_y):\n",
    "        '''Calculates and returns error based on metric of neuron'''\n",
    "        return None \n",
    "    def calc_gradient(self, y,x):\n",
    "        '''Calculates gradient of y as function wrt x'''\n",
    "        if(self.activation == sigmoid):\n",
    "            return del_sigmoid()\n",
    "        elif(y== calc_error):\n",
    "            None\n",
    "        return y(x+1e-10)-y(x)/1e-10 #taking limit to calculate gradient \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "    def set_bias(self, bias):\n",
    "        self.bias  = bias\n",
    "    def give_relevant_info(self, info): #this function serves to receive relevant information, add any variables here\n",
    "        None\n",
    "    def add_neighbour(self,neigbhour):\n",
    "        self.neighbours.append(neigbhour)\n",
    "        \n",
    "    #def set_previous_layer_activations(self, previous_layer_activations):\n",
    "     #   self.previous_layer_activations = previous_layer_activations\n",
    "        \n",
    "    #def gradient_descent(self, )\n",
    "    def update_parameters(self, previous_layer_activations):\n",
    "        '''Previous layer activations  is of shape m*n\n",
    "           Error_wrt_c is of shape m*1\n",
    "        '''\n",
    "        \n",
    "        '''CURRENTLY a single error will be sent across input parameters (weights and bias)'''\n",
    "        #1. BACKPROPOGATION TO UPDATE WEIGHTS AND BIAS WRT OVERALL COST \n",
    "        #assert(self.error_wrt_C)\n",
    "        avg_neuron_error = np.mean(self.error_wrt_C, axis =0)\n",
    "        num_examples = self.error_wrt_C.shape[0]\n",
    "        #weight_der = input_error*a_prev\n",
    "        #bias_der = input_error*1\n",
    "        bias_der = avg_neuron_error*1\n",
    "        self.bias = self.bias - self.learning_rate*self.bias\n",
    "        \n",
    "        weight_ders = np.dot(np.transpose(previous_layer_activations), self.error_wrt_C)\n",
    "        avg_weight_ders = weight_ders/num_examples\n",
    "        self.weights = self.weights - self.learning_rate*avg_weight_ders\n",
    "       # print(avg_weight_ders)\n",
    "        #lambda w,  = avg_neuron_error* \n",
    "        #for w_ in self.weights:\n",
    "         #   w_ = w_ \n",
    "            \n",
    "        \n",
    "        #2. AUXILLIARY UPDATES\n",
    "        \n",
    "    #def backward_pass(self,err): #improves weights of neuron\n",
    "     #   '''Calculates weight update based on dC/dw'''\n",
    "    #first calculate the gradient of error vs output\n",
    "       # del_C_out = \n",
    "        #then obtain del (dC/dw) =dC/dout *dout/dWi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0, -1,  0])]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = map(lambda x: x-1, x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49521257]])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the neuron which takes five inputs\n",
    "n1 = neuron(5)\n",
    "n1.get_weights().shape\n",
    "n1.forward_pass([[0.11,0.4,0.1,0.1,0.21]])\n",
    "#n1.forward_pass([[0.11,0.4,0.1,0.1,0.21],[0.11,0.4,0.1,0.1,0.21]], weighted_as_well=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a layer of independent neurons\n",
    "#Layer can have additional properties- deactivate neurons at a particular step, \n",
    "#A layer should work independently whether placed in a top down or bottom up processing \n",
    "class Layer:\n",
    "    '''\n",
    "    think of this as a GROUPING FOR MULTIPLE NEURONS--> \n",
    "    ANY CHANGES TO backprop, etc actually take place at neuron level. This only serves to clump together and pass activations in a GROUP\n",
    "    Takes in input shape, max number of neurons in layer and feedforward function to output active neurons'''\n",
    "    '''\n",
    "    Weights are stored as num_neurons*input_dims\n",
    "    Bias are stored as num_neurons*1\n",
    "    activation value is stored as m*n where m is number \n",
    "    Potential Additions :\n",
    "    1) Different number of activation layers\n",
    "    2) Active neurons changes as per need'''\n",
    "    def __init__(self, input_dims, max_num_neurons =10, input_layer= False, final_layer = False, neuron_activation_tuples = [(sigmoid,10)]):\n",
    "        '''Input_shape: batch_size* input_dims'''\n",
    "        self.dims = input_dims #input_shape[-1] #number of dims of input\n",
    "        #self.batch_size = input_shape[0] \n",
    "        self.input_layer = input_layer\n",
    "        self.final_layer = final_layer\n",
    "        self.max_num_neurons = max_num_neurons #max_num_neurons is the total number of neurons to be grouped\n",
    "        self.num_active_neurons = self.max_num_neurons\n",
    "        self.neurons = []\n",
    "        self.neuron_activations = []\n",
    "        if(neuron_activation_tuples == [(sigmoid,10)]): #default case\n",
    "            neuron_activation_tuples[0] = (sigmoid, self.max_num_neurons)\n",
    "        \n",
    "        n_index = 0     \n",
    "        for activation_tuple in neuron_activation_tuples:#adding activations\n",
    "            for i in range(n_index, n_index + activation_tuple[1]):\n",
    "                self.neurons += [neuron(self.dims, activation_tuple[0])]\n",
    "                self.neuron_activations.append(activation_tuple[0])\n",
    "            n_index = activation_tuple[1]+1 \n",
    "        self.active_neurons_index = range(len(self.neurons)) #this may be used later to switch number of active neurons    \n",
    "         \n",
    "        \n",
    "        #if(not (len(self.neuron_activations)== self.max_num_neurons) and not (self.input_layer)):\n",
    "            #print(\"Max number neurons must match number of activations specified in activation tuple argument\")\n",
    "            ##self.max_num_neurons = len(self.neuron_activations)\n",
    "            \n",
    "        #if(not final_layer):  \n",
    "         #   ''' we will use half tanh and half sigmoid here'''\n",
    "          #  self.neurons = [neuron(self.dims,sigmoid) for i in range(max_num_neurons//2)]\n",
    "           # self.neurons += [neuron(self.dims, tanh) for i in range(max_num_neurons//2, max_num_neurons)]\n",
    "            #self.active_neurons_index = range(len(self.neurons))\n",
    "            \n",
    "        #else:\n",
    "         #   inp = int(input(\"Enter 1 for softmax, 2 for sigmoid/tanh and 3 for regression\"))\n",
    "          #  self.neurons = [neuron(self.dims, activation_dict[inp]) for i in range(max_num_neurons)]\n",
    "           # self.active_neurons_index = range(len(self.neurons))\n",
    " \n",
    "        \"\"\" Below are state variables \"\"\"\n",
    "        #self.neuron_activations = [neuron_.activation for neuron_ in self.neurons]\n",
    "        if(not self.input_layer):\n",
    "            assert self.max_num_neurons == len(self.neuron_activations)\n",
    "            self.weight_matrix = self.get_weights()\n",
    "            self.bias_vector = self.get_bias()\n",
    "            \n",
    "        \n",
    "        self.inputs = []\n",
    "        self.activation = []#set as per input is given, otherwise need to account for batch size\n",
    "        self.non_activated = []\n",
    "        '''self.errors is calculated from neuron view (can include other objectives)\n",
    "           self.error_wrt_primary_C is calculated from final output view (overall cost/objective)\n",
    "        '''\n",
    "        self.errors = []\n",
    "        self.errors_wrt_primary_C = [] \n",
    "        self.del_a_z = []\n",
    "        self.previous_layer_activations = [] #We need this for standard backpropogation \n",
    "        self.next_layer = None #pointer to the next layer\n",
    "        self.prev_layer = None\n",
    "        \n",
    "    def get_layer_errors(self):\n",
    "        return self.errors\n",
    "   \n",
    "\n",
    "    def set_layer_errors_wrt_primary_C(nparray, self): \n",
    "        self.errors_wrt_primary_C = nparray\n",
    "        \n",
    "    def get_layer_errors_wrt_primary_C(self):\n",
    "        return self.errors_wrt_primary_C\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weight_matrix = np.zeros((self.max_num_neurons, self.dims))\n",
    "        for i, neuron_ in enumerate(self.neurons):\n",
    "            weight_matrix[i, :] = np.array(neuron_.get_weights())[:,0] #reshape drop second axis\n",
    "        return np.transpose(weight_matrix)\n",
    "    \n",
    "    def get_bias(self):\n",
    "        bias_vector = np.zeros((self.max_num_neurons))\n",
    "        for i, neuron_ in enumerate(self.neurons):\n",
    "            bias_vector[i] = np.array(neuron_.get_bias())\n",
    "        return np.transpose(bias_vector)\n",
    "    \n",
    "    #def set_batch_size(self, batch_size):\n",
    "     #   self.batch_size = batch_size\n",
    "        \n",
    "    def simple_feedforward(self, inputs):\n",
    "        '''USe this when multiple activations in same layer are not used and when neuron level propogation is not needed'''\n",
    "        self.inputs = inputs\n",
    "        #assert self.batch_size == self.inputs.shape[0]\n",
    "        if(self.input_layer):\n",
    "            self.activation = np.array(inputs)\n",
    "        else:\n",
    "            self.non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "            #temp = \n",
    "            #self.activated = \n",
    "            #non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "            #activated = sigmoid(non_activated[:,:3])\n",
    "            #activated2 = tanh(non_activated[:,3:])\n",
    "           # np.concatenate((activated,activated2),axis=1) # ONLY IF 1st axis corresponds to neuron \n",
    "    \n",
    "    def set_layer_error_wrt_prim_C(self, del_C_wrt_unactivated):\n",
    "        self.errors_wrt_primary_C = del_C_wrt_unactivated\n",
    "        \n",
    "    def set_previous_layer_activations(self, prev_layer_acts):\n",
    "        self.previous_layer_activations = prev_layer_acts\n",
    "        \n",
    "    def feedforward(self, inputs, weighted_as_well = True):\n",
    "        drop_second_axis = lambda x: x.reshape(x.shape[0])\n",
    "        self.inputs = inputs\n",
    "        if(self.input_layer):\n",
    "            self.activation = np.array(inputs) \n",
    "            return (np.array(inputs))\n",
    "        \n",
    "        \n",
    "        self.activation, self.non_activated, self.del_a_z = zip(*[neuron_.forward_pass(self.inputs, True) for neuron_ in self.neurons])\n",
    "        \n",
    "        self.activation = np.array(self.activation).transpose()[0]\n",
    "        self.non_activated = np.array(self.non_activated).transpose()[0]\n",
    "        self.del_a_z = np.array(self.del_a_z).transpose()[0] #should this be calculated here or taken from neuron?? \n",
    "        #print(self.del_a_z.shape)    \n",
    "            \n",
    "                                   #append every neurons output with a list -> convert to np array once done-> store parallel weighted_only list\n",
    "            #we can do this directly using the weight matrix\n",
    "            #in this case, we will use neurons index\n",
    "            #need to store activation, weighted input, \n",
    "            #return drop_second_axis()\n",
    "        return self.activation\n",
    "\n",
    "    def update_parameters_normal_layer(self):\n",
    "        \"\"\"PARRALELIZE THIS FUNCTION\"\"\"\n",
    "        '''Pass respective error to each neuron--> PARALLELIZE this '''\n",
    "        for i, n_ in enumerate(self.neurons):\n",
    "            '''WE NEED A FUNCTION TO REPLACE THIS- obtain batchwise errors/axis'''\n",
    "            n_.error_wrt_C = self.errors_wrt_primary_C[:,i] \n",
    "            n_.update_parameters(self.prev_layer.activation)\n",
    "        #print(\"Updated parameters of layer: {}\".format(self))\n",
    "        #parallel code\n",
    "        \n",
    "        #after parallel updates\n",
    "    \n",
    "    \n",
    "    def update_parameters_final_layer(self):\n",
    "        self.update_parameters_normal_layer()\n",
    "    \n",
    "    \n",
    "    def update_parameters(self):\n",
    "        if(self.final_layer):\n",
    "            self.update_parameters_final_layer()\n",
    "        else:\n",
    "            self.update_parameters_normal_layer()\n",
    "            \n",
    "        \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(1,2,3),(5,3,6)]\n",
    "y,z,a = zip(*x)\n",
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer(3, 6, False, True) # input_shape, max_num_neurons =10, input_layer= False, final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24995962,  0.20027586,  0.20492398,  0.03077423,  0.23751362,\n",
       "         0.09020697],\n",
       "       [ 0.07021748,  0.00586455,  0.23190998,  0.00067327,  0.11277822,\n",
       "         0.02663988]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.feedforward(x).shape\n",
    "l1.del_a_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.49364513,  0.27701089,  0.28768885,  0.99784698, -0.42570769,\n",
       "         0.97547097],\n",
       "       [ 0.07599231,  0.00589935,  0.36550085,  0.99999909, -0.95664707,\n",
       "         0.99841512]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "activated = sigmoid(non_activated[:,:3])\n",
    "activated2 = tanh(non_activated[:,3:])\n",
    "#np.stack([activated,activated2], axis =1)\n",
    "print(activated.shape)\n",
    "np.concatenate((activated,activated2),axis=1) #1st axis/column needs to be joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.feedforward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24995962,  0.20027586,  0.20492398,  0.03077423,  0.23751362,\n",
       "         0.09020697],\n",
       "       [ 0.07021748,  0.00586455,  0.23190998,  0.00067327,  0.11277822,\n",
       "         0.02663988]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.del_a_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.49364513],\n",
       "         [ 0.07599231]]), array([[-0.02542085],\n",
       "         [-2.49808822]]), array([[ 0.24995962],\n",
       "         [ 0.07021748]])), (array([[ 0.27701089],\n",
       "         [ 0.00589935]]), array([[-0.95933733],\n",
       "         [-5.12699636]]), array([[ 0.20027586],\n",
       "         [ 0.00586455]])), (array([[ 0.28768885],\n",
       "         [ 0.36550085]]), array([[-0.90663531],\n",
       "         [-0.55156734]]), array([[ 0.20492398],\n",
       "         [ 0.23190998]])), (array([[ 0.96821552],\n",
       "         [ 0.99932627]]), array([[ 3.41647647],\n",
       "         [ 7.30201221]]), array([[ 0.03077423],\n",
       "         [ 0.00067327]])), (array([[ 0.38825753],\n",
       "         [ 0.12956542]]), array([[-0.45464253],\n",
       "         [-1.9048067 ]]), array([[ 0.23751362],\n",
       "         [ 0.11277822]])), (array([[ 0.89974121],\n",
       "         [ 0.9726099 ]]), array([[ 2.19435243],\n",
       "         [ 3.56980152]]), array([[ 0.09020697],\n",
       "         [ 0.02663988]]))]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[neuron_.forward_pass(x, weighted_as_well= True) for neuron_ in l1.neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict = {1:sigmoid, 2: tanh, 3: weighted_linear, 4: relu, 5: linear}\n",
    "class Artificial_Neural_Network:\n",
    "    def __init__(self, num_neurons_in_layer = [(5),(3,[(sigmoid, 2),(tanh, 1)]), (1,[(sigmoid, 1)])], cost_func = mse):\n",
    "        self.num_layers = len(num_neurons_in_layer)\n",
    "        self.num_neurons_in_each_layer = num_neurons_in_layer\n",
    "        self.input_dims = num_neurons_in_layer[0]\n",
    "        self.output_dims = num_neurons_in_layer[-1]\n",
    "        self.layers = []\n",
    "\n",
    "        self.cost_func = cost_func\n",
    "        self.layers.append(Layer(num_neurons_in_layer[0], num_neurons_in_layer[0],True)) #input layer\n",
    "        prv_layer_neurons = num_neurons_in_layer[0]\n",
    "        for i, layer_tuple in enumerate(num_neurons_in_layer[1:]):\n",
    "            if(not (i==len(num_neurons_in_layer)-2)): #since we started at two\n",
    "                self.layers.append(Layer(prv_layer_neurons, layer_tuple[0], False, False, layer_tuple[1] ))\n",
    "            else: #last layer/ output layer\n",
    "                self.layers.append(Layer(prv_layer_neurons, layer_tuple[0], False, True, layer_tuple[1] ))\n",
    "            self.layers[i].next_layer = self.layers[i+1] \n",
    "            self.layers[i+1].prev_layer = self.layers[i]\n",
    "            prv_layer_neurons = layer_tuple[0]\n",
    "            \n",
    "#def __init__(self, input_dims, max_num_neurons =10, input_layer= False, final_layer = False,\n",
    "#neuron_activation_tuples = [(sigmoid,10)] )\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        '''\n",
    "        Input of shape m*n_input_layer\n",
    "        Output of shape m*n_final_layer\n",
    "        '''\n",
    "        forward_val = inputs\n",
    "        for layer_ in self.layers:\n",
    "            layer_.set_previous_layer_activations(forward_val)\n",
    "            forward_val = layer_.feedforward(forward_val)\n",
    "            \n",
    "        return forward_val\n",
    "    \n",
    "    \n",
    "    def backprop(self, der_C_wrt_prediction):\n",
    "        '''Input of shape m*n final_layer\n",
    "           No outputs, only updates layer and neuron wise\n",
    "        '''\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(layer.final_layer):\n",
    "                #print(der_C_wrt_prediction.shape)\n",
    "                #print(layer.del_a_z)\n",
    "                assert der_C_wrt_prediction.shape == layer.del_a_z.shape\n",
    "                del_C_wrt_unactivated = np.multiply(der_C_wrt_prediction, layer.del_a_z) #ELEMENT WISE\n",
    "                layer.set_layer_error_wrt_prim_C(del_C_wrt_unactivated)\n",
    "                #print(\"DEL_C\")\n",
    "                #print(del_C_wrt_unactivated)\n",
    "                layer.update_parameters()\n",
    "                upper_layer_error = del_C_wrt_unactivated\n",
    "            \n",
    "            elif(layer.input_layer):\n",
    "                continue #DO nothing here\n",
    "            \n",
    "            else:\n",
    "                #del_C.... is of shape m*n and layer (the upper layer from which error is propogated) weights are of shape k*n \n",
    "                del_C_wrt_activated = np.dot(upper_layer_error, np.transpose(layer.next_layer.get_weights()))   #modify weights after backpropogating error to previous layer\n",
    "                del_C_wrt_unactivated = np.multiply(del_C_wrt_activated, layer.del_a_z)\n",
    "                #layer.del_a_z = del_C_wrt_unactivated #\n",
    "                #print(del_C_wrt_unactivated.shape)\n",
    "                layer.set_layer_error_wrt_prim_C(del_C_wrt_unactivated) \n",
    "                layer.update_parameters() #update parameters by calculating del_C_wrt_parameter\n",
    "                #update layer weights\n",
    "                upper_layer_error = del_C_wrt_unactivated\n",
    "                \n",
    "            #del_C_wrt_unactivated =   #IMP for now a neuron's error is not equaivalent to layer's index error\n",
    "            #layer.set_layer_error() \n",
    "            \n",
    "        \n",
    "    def cost_calculation(self, y_true, y_pred):\n",
    "        return self.cost_func(y_true,y_pred)\n",
    "    \n",
    "    def training(self,x,y, print_metrics = True):\n",
    "        '''This takes in x and y and updates weights after processing entire input'''\n",
    "        '''For given x vector or matrix (M*N) where m is the number of tikraining examples, and y is the expected output (M*Y_dim):\n",
    "        1) Calculate forward pass (obtain resultant activations)\n",
    "        2) Obtain the associated cost with forward pass\n",
    "        3a) Adjust weights and biases depending on cost (CURRENTLY using gradient descent)\n",
    "        3b) Backpropogate error/cost through chain rule through each layer\n",
    "        3c) Define error as delC/del(prev_activation)--> use this to calculate delC/delW --> delC/del_prev_A* del_prev_A/delW\n",
    "        Update each weight and bias based on backpropogation\n",
    "        \n",
    "        Output of shape \n",
    "        \n",
    "        '''\n",
    "        if(x.ndim ==1): #only 1 training example provided (stochastic)\n",
    "            x = x.reshape(1, x.shape[0])\n",
    "            \n",
    "        #get forward pass predictions \n",
    "        forward_pass_output = self.feedforward(x)\n",
    "        #predictions = \n",
    "        predictions = forward_pass_output\n",
    "        \n",
    "        \n",
    "        #perform error calculation of relevant terms\n",
    "        assert y.shape == predictions.shape\n",
    "        avg_error, der_C_wrt_prediction = self.cost_calculation(y, predictions)\n",
    "        \n",
    "        if(print_metrics):\n",
    "            print(\"Predicted value: {} \".format(predictions))\n",
    "            print(\"Average batch cost: {}\".format(avg_error))\n",
    "        #print(der_C_wrt_prediction.shape)\n",
    "        #perform parameter updates by calling relevant functions\n",
    "        self.backprop(der_C_wrt_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = Artificial_Neural_Network([(2),(10,[(sigmoid, 5), (tanh, 5)]), (2,[(sigmoid, 1), (tanh, 1)]), (2,[(sigmoid, 2)])], cost_func= cross_entropy)\n",
    "#ANN = Artificial_Neural_Network([(3),(200,[(sigmoid, 120),(tanh, 80)]), (1,[(sigmoid, 1)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.array([[0.5,0.8],[0.8,0.5],[0.5,0.8],[0.8,0.5]])\n",
    "xor_true_values = np.array([[1,0],[1,0],[0,1],[0,1]])\n",
    "input_values = np.array([[0.1, 0.1, 0.1],[0.9,0.9,0.9],[0.1, 0.1, 0.1],[0.9,0.9,0.9]])\n",
    "xor_input_values = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "#input_values, predicted_values = \n",
    "#ANN.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-89181e481b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mANN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxor_input_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxor_true_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-298-82bfd65091f3>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, x, y, print_metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#perform error calculation of relevant terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mavg_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mder_C_wrt_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprint_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    ANN.training(xor_input_values, xor_true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN.layers[0].del_a_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(np.array([[2,3,3],[1,2,3]]),np.array([[1,2,3],[1,1,3]]))[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 84.64883509,  92.16980258],\n",
       "       [ 89.35310607,  97.24259667]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN.layers[-1].feedforward(ANN.layers[1].feedforward(ANN.layers[0].feedforward(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To do:\n",
    "1) How to take activation inputs from user for NNet class DONE\n",
    "2) Check final forward propogation DONE\n",
    "3) Do gradient descent backprop at neuron level DONE\n",
    "'''\n",
    "\n",
    "'''Fun stuff to do\n",
    "1) Alzheimer neuron-> as mentioned by Bobby\n",
    "2) Genetic neurons --> Each neuron is represented by some gene which makes it behave a particular way. Question is: Should genes be precoded or genes will be learned as training progresses. Similar gene neurons behave in simlar ways \n",
    "3) Is a cost function the best way to judge a nnets performance? What is the best way to determine the best configuration of weights and errors. Is it wrong to learn through gradients? \n",
    "'''\n",
    "\n",
    "'''Stuff to do:\n",
    "VI: <B> ADD CROSS entropy </B>\n",
    "1) Parrallelize update parameters for layer class\n",
    "2) Generalize backpropogate so that axis for each --> batch, neurons, time etc can be represented\n",
    "3) Autoencoder\n",
    "4) RNN\n",
    "5) \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To do:\n",
    "1) Insert hebbian learner--> calculate associative strength of neurons, if two neurons co-occur then make a weight between the two\n",
    "2) Insert a redundant learner/ Alzheimer's neuron --> mantain a self.last_10activations list, see rate of change of input-> last_10_inputs, EXTEND the base neuron class \n",
    "5) VVVVIP: Parallelize the update parameters in Layer class for each neuron  \n",
    "3) ADD a variable in code to accept axis of error and axis of other things \n",
    "4) Generalize the backpropogate function in layer to specify axis of error\n",
    "LINE 140 Parallelize it\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
