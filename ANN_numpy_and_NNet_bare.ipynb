{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some dependencies for making a neural network \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0 \n",
    "\n",
    "## Preliminaries, descriptions and questions\n",
    "\n",
    "1. Goal of this particular neural network: Extract patterns from given observation inspired by human brain functioning. Note that inspiration does not equal representation, ie, neural network is not an exact replication of cognitive processing. Also, I like to view a neural network may as a possible strategy to form robust representations for AI. For me, it is a machine learning technique that can play a big role in AI. I have presented the below thoughts in context of AI. \n",
    "\n",
    "### Disclaimer: All thoughts below are my own interpretation and may not be an accurate representation of literature. \n",
    "2. Given observations are currently represented as vectors of information (matrices, vectors seem the most convenient and intuitive way). \n",
    "    1. In a 'supervised setting' each/some observation has a corresponding outcome/label. \n",
    "    2. Personally, I am intrigued by the process of feeding input data to an AI system. Is a streams of vectors (a matrix)the correct approach? What mechanism does the human brain use to feed in data through sensory agents? \n",
    "        1. Humans and living beings have two crucial mechanisms to obtain 'input data'- sensory agents and interactive agents. <b> Q: How does the environmental interaction and sensing play crucial roles in influencing cognitive processes?</b> \n",
    "        2. The environment surrounding the interaction/sense may also influence the cognitive process/output. The result too could be stored for recall later (memory). I am not aware of the exact mechanism of how memory is recalled or stored. <b> Again, this is an interesting area to explore. Should inputs be represented as a tuple of (input, environment, sensor_type, memory/recall, dependency on other inputs), or perhaps memory should be decoupled and part of processing unit as a current_state (hint of an FSM)? </b>\n",
    "        3. A particular interesting example of how environment and memory influence cognitive processing is 'priming.' Semantic priming influences how one may process a stream of words differently depending on previous exposure (memory) or the current environment. Thus for text tasks, I feel representing and including information on the above would be beneficial. <b>Now, taking this example forward, how can we form a 'representation' given some data? This is what neural networks/ deep learning may be used to do. Different architectures can be postulated to form more accurate/useful representations. From what I've experienced, forming more robust architectures for particular domains/problems is the current trend and approach to creating intelligent and thoughtful machines. However, this may lead to domain specific architectures, but again perhaps all may be collaborated to form an AI system.<i>Personally, I find it to be a good practice to keep probing how the problem and model we are working on  contributes to intelligent machines. </i></b> Sometimes we can create great pattern learning models which have high accuracy in tasks but not understand why they work. This is a current research question in deep learning.\n",
    "        \n",
    "        \n",
    "        \n",
    "    3. Depending on task, we may represent input data (observations) differently:\n",
    "        1. Example: Images: Represent them as pixel intensities\n",
    "        2. Example 2: Documents: Global sense: Word vectors- represent each word by context features\n",
    "        3. Example 3: ....\n",
    "\n",
    "3. <b>A concise (not exhaustive) summary/listing of primary components of human brain processing</b> that I found useful (will add proper academic papers and verified theory later) : http://www.teach-nology.com/teachers/methods/info_processing/ Key points:\n",
    "    1. Input through different stimuli and encoded storage in different modes- structural, phonemic, semantic \n",
    "    2. Transformation algorithms- bottom up processing and top down processing. THIS Is what we'll explore with neural networks. \n",
    "    3. Attention filter for signals and selection of relevant cognitive processes\n",
    "    4. Short term memory: Electric signal loop through certain neurons; Long term memory: Protein structures\n",
    "    5. Organization of knowledge in brain: Many postulated models\n",
    "    6. Retrieval of memory/recall: retrieval cue, priming, distortions. <b>How are words formed for conveying a particular thought? </b>\n",
    "    7. <b>How is emotion implemented without language, just senses?</b> \n",
    "    \n",
    "   <b>Chollet: <i>The ability to convey intent and emotions through voice seems like a universal constant that predates language. Ever noticed how the tone you'd use to ask a question, or give an order, is the same in essentially every language?</i></b>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1: Vanilla Neural network construction\n",
    "\n",
    "## The purpose of this notebook\n",
    "\n",
    "This notebook explores part 2B of the above: How to create transformation algorithms that transform inputs into more useful representations for given objectives. \n",
    "\n",
    "Additionally, it provides a foundation/accustoms one to effectively adapt new biological/mathematical concepts into code. \n",
    "\n",
    "The notebook starts with a simple neural network model- Artificial Neural Network- and then explores variants of the same. It also covers necessary mathematical and comp sci concepts in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data- assume a matrix. Each row represents a particular vector which represents a set of observations.\n",
    "#Assuming we process parallely and independently, we can process multiple input events together. This, we call a batch of inputs which will yield a batch of outputs. We will discuss non-independent batches later. \n",
    "\n",
    "x= np.array([[1,2,3],[4,5,6]])\n",
    "y= np.array([[1,10],[2,4]]) # we want two ouputs for each set of observations\n",
    "#Feel free to replace the above x_in and y with any dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our standard neural network aims to do the following (bold parts are crucial): \n",
    "\n",
    "1. Given set of inputs and outputs, find the best transformation of inputs (possibly and usually, multiple transformations) to obtain outputs. This it does through 'weighting each previous input then applying a non-linearity to the weighted input (other mechanisms- perhaps biological inspired activations, not necessarily 'non-linear', can be used)\n",
    "\n",
    "2. In finding the best transformations, we are finding how to weight certain data. Given a lot of data, we are eventually extracting recurring patterns in the data. \n",
    "\n",
    "3. The best transformations are dependent on weights for each neuron (other variables can be included, but proper mechanisms need to be developed for how to modify the variables for best usage. Ex: backprop/gradient passing is used for finding best weights. However, it is disputed whether this is the best mechanism to adjust weights.)\n",
    "\n",
    "4. In this particular ANN by using backprop/gradient passing we will try to find (not guaranteed to be optimum-mathematically or biologically) the best weights/variables to satisfy a given objective/cost function. There are sophisticated mathematical tweaks and improvements on passing of gradients and the optimization process.  Good overview: https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f \n",
    "\n",
    "5. The cost function plays the role of a metric to see whether our model is outputting satisfactory values (DO \n",
    "HUMANS have cost functions?)\n",
    "\n",
    "6. <b>New variabes can be added in this paradigm- and to adjust them, we need to derive and implement the gradient with respect to the cost function. </b>\n",
    "> Ex: An example variable can be associated primes to a particular input.  \n",
    "\n",
    "<b>Otherwise new activation/feedforward computation mechanisms, variable-adjusting mechanisms and objective functions- specific to the variable- need to be proposed and implemented. </b> \n",
    "\n",
    ">Ex: In an hebbian learner the variable is co-occuring neurons, objective is unknown, but mechanism is to increase weights/strength of connection whenever there is co-occurance.\n",
    "\n",
    "7. <b>For every new variable, think of three things- \n",
    "    1. How does it impact forward value calculation.\n",
    "    2. How should we adjust the variable (if needed) in light of correctness/objective function\n",
    "    3. What objective function (if any) needs to be used</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a standard neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The below focuses on creating a neural network from a neuron level. The focus is not entirely on matrix transformations and layer wise transformations, but also on how particular neurons may act differently in the same computation step.\n",
    "##### Hence, this is not meant to be a scalable/efficient network, but hopefully a playground to create slightly different neural network architectures.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[4,3,2],[1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "np.random.seed(5)\n",
    "##Auxiliary functions \n",
    "\n",
    "def visual_plot(x,y):\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    \n",
    "def weighted_linear(x,W=1,der= False, visual=False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), weighted_linear(np.arange(-10,10,0.1),1))\n",
    "    if(not der):\n",
    "        return x #np.dot(x,np.transpose(W)) #actually it should just be x since we're assuming weights are computed in prior step\n",
    "    else:\n",
    "        return x/x #this should not be 1, think why\n",
    "\n",
    "def linear(x, der = False): #incase of confusion\n",
    "    return weighted_linear(x, W=1, der=der)\n",
    "\n",
    "def sigmoid(z, der=False, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), sigmoid(np.arange(-10,10,0.1)))\n",
    "    if(der):\n",
    "        return der_sigmoid(z)\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def der_sigmoid(z, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), del_sigmoid(np.arange(-10,10,0.1)))\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu(z,visual = False):\n",
    "    return np.array(map(lambda x: max(0,x)),z)\n",
    "    \n",
    "def tanh(z, der = False, visual = False):\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-10,10,0.1), tanh(np.arange(-10,10,0.1)))\n",
    "    if(der):\n",
    "        return der_tanh(z)\n",
    "    return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "\n",
    "def der_tanh(z, visual = False):\n",
    "    return 1- np.power(tanh(z),2)\n",
    "\n",
    "'''\n",
    "def mse(correct_y, predicted_y, der = False, visual = False):\n",
    "    #calculates mse cost given predicted y vector and correct y vector for a single training example\n",
    "    \n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-5,10,0.1),np.power([5]-np.arange(-5,10,0.1),2)/2)\n",
    "    if(not der): #mse is calculated as 1/2n* (squared difference)\n",
    "        return np.mean(np.power(correct_y-predicted_y,2), axis=0)/(2) #average over rows\n",
    "        return np.power(correct_y-predicted_y, 2)/2\n",
    "    else:\n",
    "        #derivative with respect to predicted_y\n",
    "        return np.mean(predicted_y-correct_y, axis =0) # average\n",
    "        return predicted_y - correct_y\n",
    "    \n",
    "'''\n",
    "\n",
    "def l2_norm(x, vector_axis=1):\n",
    "    \"\"\"x is an input tensor\n",
    "    output is l2 norm of input_shape_with_last_axis=1\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(x,2), axis =vector_axis))\n",
    "    \n",
    "def mse(correct_y, predicted_y, der_and_avg= True, der = False, visual = False):\n",
    "    '''\n",
    "    mse(y,y') = (1/2m) * sum_across_examples(y-y')^2 where m is batch size/number of training examples \n",
    "    calculates mse cost given predicted y matrix and correct y' matrix for a given batch'''\n",
    "    '''If inputs are of shape m*n then output is of tuple (m*n, 1*n, m*n)'''\n",
    "    assert correct_y.shape == predicted_y.shape\n",
    "    \n",
    "    if(visual):\n",
    "        visual_plot(np.arange(-5,10,0.1),np.power([5]-np.arange(-5,10,0.1),2)/2)\n",
    "    \n",
    "    diff = predicted_y - correct_y\n",
    "    m = predicted_y.shape[0]\n",
    "    \n",
    "    if(der_and_avg):\n",
    "        der_c_wrt_pred_y = diff/m ##m*n of derivatives \n",
    "        mse_individual = np.power(diff,2) \n",
    "        mse_avg_across_output_dim = np.average(mse_individual, axis =1)\n",
    "        #mse_individual = l2_norm(diff) #m*1 mse l2 norm for each example\n",
    "        mse_avg = np.mean(mse_avg_across_output_dim, axis =0)/2 #1*1 mse avg according to formula\n",
    "        return mse_avg, der_c_wrt_pred_y \n",
    "    else:    \n",
    "        if(not der): #just the avg cost\n",
    "            mse_individual = np.power(diff,2)#l2_norm(diff)\n",
    "            return np.mean(mse_individual)/2 # mse_individual  #THIS IS 1*n (averaged across batch) \n",
    "        #return np.sum(np.power(correct_y-predicted_y,2))/(correct_y.shape[0]*2)\n",
    "        else:#just the cost derivate\n",
    "            return diff/m  #np.mean(diff, axis =0) #THIS IS m*n, derivative of mse with respect to prediction\n",
    "\n",
    "\n",
    "def softmax(z,der=False, visual=False):\n",
    "    '''Return a more uniform (gibbs) distribution over inputs'''\n",
    "    e_to_x = np.exp(z)\n",
    "    e_to_x_sum = np.sum(e_to_x, axis = 1)\n",
    "    softmaxed_out = e_to_x/e_to_x_sum\n",
    "    if(not der):\n",
    "        return softmaxed_out\n",
    "    else:\n",
    "        return softmaxed_out,  c           \n",
    "    \n",
    "def bitwise_cross_entropy(correct_y, predicted_y, der=False, visual= False):\n",
    "    '''correct_y is of shape m*n where n = ceil(lg(num_outputs))\n",
    "    Use this when neurons in output layer is bitwise (not one hot) encoded\n",
    "    '''\n",
    "    None\n",
    "    \n",
    "    \n",
    "def cross_entropy(correct_y, predicted_y, der=False, visual = False): #assume correct_y is a scalar for each training example\n",
    "    '''\n",
    "    correct_y is a one hot encoded array of shape m*n \n",
    "    predicted_y is vector of output probabilies(softmax)\n",
    "    if conditions are used in case a single dimensional vector/single point input is given\n",
    "    '''\n",
    "    if(visual):\n",
    "        visual_plot(np.arange(0,1,0.01), -np.log(np.arange(0,1,0.01)))\n",
    "        #visual_plot(np.arange(-10,10,0.1))\n",
    "    \n",
    "    if(correct_y.ndim>1):\n",
    "        indices = np.argmax(correct_y,axis=1) #correct_y example [[[0,0,1,0],[0,0,0,1]]] given four classes and two training examples \n",
    "        predicted_prob_given_indices = map(lambda y,ind: y[ind], predicted_y, indices) #select prob values of predictions given the index\n",
    "        num_inputs = correct_y.shape[0]\n",
    "    else:\n",
    "        #stochastic example\n",
    "        indices = np.array(np.argmax(correct_y))\n",
    "        predicted_prob_given_indices = predicted_y[indices]\n",
    "        num_inputs = 1\n",
    "    return np.sum(-np.log(predicted_prob_given_indices))/num_inputs\n",
    "\n",
    "def hadamard_product(x,y): #element wise product\n",
    "    return np.multiply(x,y)\n",
    "\n",
    "activation_dict = {1:sigmoid, 2: tanh, 3: weighted_linear, 4: relu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.7]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0,0.2,0.8,0],[0,0.1,0.2,0.7]]\n",
    "ind = np.argmax([[0,0,1,0],[0,0,0,1]], axis=1)\n",
    "#predicted_prob_given_indices = map(lambda y,ind: y[ind],list(zip(y, ind)))\n",
    "map(lambda x,a: x[a], y,ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember the purpose of this neural network is to transform data into a better representation in a 'bottom up' approach. \n",
    "#A transformation is simply a function of x: f(x). While there exist many different forms/models of transformation functions- linear weights, polynomials, any function one thinks will help\n",
    "\n",
    "#A neuron, however, is modelled based on its biological components. Here's a good summary: https://www.khanacademy.org/science/biology/human-biology/neuron-nervous-system/a/overview-of-neuron-structure-and-function. A typical neuron consists of the following:\n",
    "#1) Neurons form a network- a top down processing network leads to later neurons forming more abstract representations\n",
    "#2) Neurons have activation signals which influence other activation signals\n",
    "#3) A neuron might weight each 'synaptic input' differently\n",
    "#4) Each neuron represents a particular representation. \n",
    "#5) Remember in this architecture, each neuron represents a value between 0 and 1 (non-fuzzy) value. The first layer neurons correspond to the non-complex representations, which become more complex as they activate further neurons. \n",
    "#I have used an OOP approach since it would be interesting to model different type of neurons. However, for scalable implementations a bunch of layer wise matrices should be used.  \n",
    "#6) We will explore adding a new variable as well\n",
    "class neuron:\n",
    "    \n",
    "    def __init__(self, input_size,activation = sigmoid, lr= 0.1, belongs_to = None):\n",
    "        self.shape = (input_size,1)\n",
    "        self.weights = np.random.random(self.shape) - np.random.random(self.shape) #np.random.random(self.shape) #initialize random values  #np.zeros(self.shape)#\n",
    "        self.bias = np.array([0])\n",
    "        self.activation = activation\n",
    "        self.learning_rate = lr\n",
    "        self.surrounding_activations = None\n",
    "        self.belongs_to = belongs_to\n",
    "        self.neighbours = []\n",
    "        #self.previous_layer_activations = []\n",
    "        self.error_wrt_C = None\n",
    "        self.historical_activations = None #Hebbian learner to mantain co-occuring high activations together-> if n1 and n2 co-occur a lot, then if n1 occurs add a bias term?\n",
    "    def forward_pass(self,input_x, weighted_as_well = False): #weighted as well means just weighted, not activated\n",
    "        '''Input x is of shape m*n where m is batch size and n is input dims'''\n",
    "        weighted_input = np.dot(input_x, self.weights)+self.bias\n",
    "        if(weighted_as_well): #this returns both activated and non activated output \n",
    "            return (self.activation(weighted_input),weighted_input, self.activation(weighted_input, der = True)) \n",
    "        else:\n",
    "            return self.activation(weighted_input)\n",
    "    def adjust_parameters(self):\n",
    "        None\n",
    "    def calc_error(self,correct_y, predicted_y):\n",
    "        '''Calculates and returns error based on metric of neuron'''\n",
    "        return None \n",
    "    def calc_gradient(self, y,x):\n",
    "        '''Calculates gradient of y as function wrt x'''\n",
    "        if(self.activation == sigmoid):\n",
    "            return del_sigmoid()\n",
    "        elif(y== calc_error):\n",
    "            None\n",
    "        return y(x+1e-10)-y(x)/1e-10 #taking limit to calculate gradient \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "    def set_bias(self, bias):\n",
    "        self.bias  = bias\n",
    "    \n",
    "    def set_error_wrt_C(self, error_wrt_C):\n",
    "        sefl.error_wrt_C = error_wrt_C\n",
    "    def give_relevant_info(self, info): #this function serves to receive relevant information, add any variables here\n",
    "        None\n",
    "    def add_neighbour(self,neigbhour):\n",
    "        self.neighbours.append(neigbhour)\n",
    "        \n",
    "    #def set_previous_layer_activations(self, previous_layer_activations):\n",
    "     #   self.previous_layer_activations = previous_layer_activations\n",
    "        \n",
    "    #def gradient_descent(self, )\n",
    "    def update_parameters(self, previous_layer_activations):\n",
    "        '''Previous layer activations  is of shape m*n\n",
    "           Error_wrt_c is of shape m*1\n",
    "        '''\n",
    "        \n",
    "        '''CURRENTLY a single error will be sent across input parameters (weights and bias)'''\n",
    "        #1. BACKPROPOGATION TO UPDATE WEIGHTS AND BIAS WRT OVERALL COST \n",
    "        #assert(self.error_wrt_C)\n",
    "       # print(\"del_C_wrt_del_z\",self.error_wrt_C)\n",
    "        avg_neuron_error = np.mean(self.error_wrt_C, axis =0)\n",
    "        num_examples = self.error_wrt_C.shape[0]\n",
    "        #weight_der = input_error*a_prev\n",
    "        #bias_der = input_error*1\n",
    "        bias_der = avg_neuron_error*1\n",
    "        self.bias = self.bias - self.learning_rate*self.bias\n",
    "        \n",
    "        weight_ders = np.dot(np.transpose(previous_layer_activations), self.error_wrt_C)\n",
    "        avg_weight_ders = weight_ders/num_examples\n",
    "        self.weights = self.weights - self.learning_rate*avg_weight_ders\n",
    "       # print(avg_weight_ders)\n",
    "        #lambda w,  = avg_neuron_error* \n",
    "        #for w_ in self.weights:\n",
    "         #   w_ = w_ \n",
    "            \n",
    "        \n",
    "        #2. AUXILLIARY UPDATES\n",
    "        \n",
    "    #def backward_pass(self,err): #improves weights of neuron\n",
    "     #   '''Calculates weight update based on dC/dw'''\n",
    "    #first calculate the gradient of error vs output\n",
    "       # del_C_out = \n",
    "        #then obtain del (dC/dw) =dC/dout *dout/dWi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52328645]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the neuron which takes five inputs\n",
    "n1 = neuron(5)\n",
    "n1.get_weights().shape\n",
    "n1.forward_pass([[0.11,0.4,0.1,0.1,0.21]])\n",
    "#n1.forward_pass([[0.11,0.4,0.1,0.1,0.21],[0.11,0.4,0.1,0.1,0.21]], weighted_as_well=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a layer of independent neurons\n",
    "#Layer can have additional properties- deactivate neurons at a particular step, \n",
    "#A layer should work independently whether placed in a top down or bottom up processing \n",
    "class Layer:\n",
    "    '''\n",
    "    think of this as a GROUPING FOR MULTIPLE NEURONS--> \n",
    "    ANY CHANGES TO backprop, etc actually take place at neuron level. This only serves to clump together and pass activations in a GROUP\n",
    "    Takes in input shape, max number of neurons in layer and feedforward function to output active neurons'''\n",
    "    '''\n",
    "    Weights are stored as num_neurons*input_dims\n",
    "    Bias are stored as num_neurons*1\n",
    "    activation value is stored as m*n where m is number \n",
    "    Potential Additions :\n",
    "    1) Different number of activation layers\n",
    "    2) Active neurons changes as per need'''\n",
    "    def __init__(self, input_dims, max_num_neurons =10, input_layer= False, final_layer = False, neuron_activation_tuples = [(sigmoid,10)]):\n",
    "        '''Input_shape: batch_size* input_dims'''\n",
    "        self.dims = input_dims #input_shape[-1] #number of dims of input\n",
    "        #self.batch_size = input_shape[0] \n",
    "        self.input_layer = input_layer\n",
    "        self.final_layer = final_layer\n",
    "        self.max_num_neurons = max_num_neurons #max_num_neurons is the total number of neurons to be grouped\n",
    "        self.num_active_neurons = self.max_num_neurons\n",
    "        self.neurons = []\n",
    "        self.neuron_activations = []\n",
    "        if(neuron_activation_tuples == [(sigmoid,10)]): #default case\n",
    "            neuron_activation_tuples[0] = (sigmoid, self.max_num_neurons)\n",
    "        \n",
    "        n_index = 0     \n",
    "        for activation_tuple in neuron_activation_tuples:#adding activations\n",
    "            for i in range(n_index, n_index + activation_tuple[1]):\n",
    "                self.neurons += [neuron(self.dims, activation_tuple[0])]\n",
    "                self.neuron_activations.append(activation_tuple[0])\n",
    "            n_index = activation_tuple[1]+1 \n",
    "        self.active_neurons_index = range(len(self.neurons)) #this may be used later to switch number of active neurons    \n",
    "         \n",
    "        \n",
    "        #if(not (len(self.neuron_activations)== self.max_num_neurons) and not (self.input_layer)):\n",
    "            #print(\"Max number neurons must match number of activations specified in activation tuple argument\")\n",
    "            ##self.max_num_neurons = len(self.neuron_activations)\n",
    "            \n",
    "        #if(not final_layer):  \n",
    "         #   ''' we will use half tanh and half sigmoid here'''\n",
    "          #  self.neurons = [neuron(self.dims,sigmoid) for i in range(max_num_neurons//2)]\n",
    "           # self.neurons += [neuron(self.dims, tanh) for i in range(max_num_neurons//2, max_num_neurons)]\n",
    "            #self.active_neurons_index = range(len(self.neurons))\n",
    "            \n",
    "        #else:\n",
    "         #   inp = int(input(\"Enter 1 for softmax, 2 for sigmoid/tanh and 3 for regression\"))\n",
    "          #  self.neurons = [neuron(self.dims, activation_dict[inp]) for i in range(max_num_neurons)]\n",
    "           # self.active_neurons_index = range(len(self.neurons))\n",
    " \n",
    "        \"\"\" Below are state variables \"\"\"\n",
    "        #self.neuron_activations = [neuron_.activation for neuron_ in self.neurons]\n",
    "        if(not self.input_layer):\n",
    "            assert self.max_num_neurons == len(self.neuron_activations)\n",
    "            self.weight_matrix = self.get_weights()\n",
    "            self.bias_vector = self.get_bias()\n",
    "            \n",
    "        \n",
    "        self.inputs = []\n",
    "        self.activation = []#set as per input is given, otherwise need to account for batch size\n",
    "        self.non_activated = []\n",
    "        '''self.errors is calculated from neuron view (can include other objectives)\n",
    "           self.error_wrt_primary_C is calculated from final output view (overall cost/objective)\n",
    "        '''\n",
    "        self.errors = []\n",
    "        self.errors_wrt_primary_C = [] \n",
    "        self.del_a_z = []\n",
    "        self.previous_layer_activations = [] #We need this for standard backpropogation \n",
    "        self.next_layer = None #pointer to the next layer\n",
    "        self.prev_layer = None\n",
    "        \n",
    "    def get_layer_errors(self):\n",
    "        return self.errors\n",
    "   \n",
    "\n",
    "    def set_layer_errors_wrt_primary_C(nparray, self): \n",
    "        self.errors_wrt_primary_C = nparray\n",
    "        \n",
    "    def get_layer_errors_wrt_primary_C(self):\n",
    "        return self.errors_wrt_primary_C\n",
    "    \n",
    "    def get_weights(self):\n",
    "        weight_matrix = np.zeros((self.max_num_neurons, self.dims))\n",
    "        for i, neuron_ in enumerate(self.neurons):\n",
    "            weight_matrix[i, :] = np.array(neuron_.get_weights())[:,0] #reshape drop second axis\n",
    "        return np.transpose(weight_matrix)\n",
    "    \n",
    "    def get_bias(self):\n",
    "        bias_vector = np.zeros((self.max_num_neurons))\n",
    "        for i, neuron_ in enumerate(self.neurons):\n",
    "            bias_vector[i] = np.array(neuron_.get_bias())\n",
    "        return np.transpose(bias_vector)\n",
    "    \n",
    "    #def set_batch_size(self, batch_size):\n",
    "     #   self.batch_size = batch_size\n",
    "        \n",
    "    def simple_feedforward(self, inputs):\n",
    "        '''USe this when multiple activations in same layer are not used and when neuron level propogation is not needed'''\n",
    "        self.inputs = inputs\n",
    "        #assert self.batch_size == self.inputs.shape[0]\n",
    "        if(self.input_layer):\n",
    "            self.activation = np.array(inputs)\n",
    "        else:\n",
    "            self.non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "            #temp = \n",
    "            #self.activated = \n",
    "            #non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "            #activated = sigmoid(non_activated[:,:3])\n",
    "            #activated2 = tanh(non_activated[:,3:])\n",
    "           # np.concatenate((activated,activated2),axis=1) # ONLY IF 1st axis corresponds to neuron \n",
    "    \n",
    "    def set_layer_errors_wrt_primary_C(self, del_C_wrt_unactivated):\n",
    "        self.errors_wrt_primary_C = del_C_wrt_unactivated\n",
    "        \n",
    "    def set_previous_layer_activations(self, prev_layer_acts):\n",
    "        self.previous_layer_activations = prev_layer_acts\n",
    "        \n",
    "    def feedforward(self, inputs, weighted_as_well = True):\n",
    "        drop_second_axis = lambda x: x.reshape(x.shape[0])\n",
    "        self.inputs = inputs\n",
    "        if(self.input_layer):\n",
    "            self.activation = np.array(inputs) \n",
    "            return (np.array(inputs))\n",
    "        \n",
    "        \n",
    "        self.activation, self.non_activated, self.del_a_z = zip(*[neuron_.forward_pass(self.inputs, True) for neuron_ in self.neurons])\n",
    "        \n",
    "        self.activation = np.array(self.activation).transpose()[0]\n",
    "        self.non_activated = np.array(self.non_activated).transpose()[0]\n",
    "        self.del_a_z = np.array(self.del_a_z).transpose()[0] #should this be calculated here or taken from neuron?? \n",
    "        #print(self.del_a_z.shape)    \n",
    "            \n",
    "                                   #append every neurons output with a list -> convert to np array once done-> store parallel weighted_only list\n",
    "            #we can do this directly using the weight matrix\n",
    "            #in this case, we will use neurons index\n",
    "            #need to store activation, weighted input, \n",
    "            #return drop_second_axis()\n",
    "        return self.activation\n",
    "\n",
    "    def update_parameters_normal_layer(self):\n",
    "        \"\"\"PARRALELIZE THIS FUNCTION\"\"\"\n",
    "        '''Pass respective error to each neuron--> PARALLELIZE this '''\n",
    "        for i, n_ in enumerate(self.neurons):\n",
    "            '''WE NEED A FUNCTION TO REPLACE THIS- obtain batchwise errors/axis'''\n",
    "            n_.error_wrt_C = self.errors_wrt_primary_C[:,i] \n",
    "            n_.update_parameters(self.prev_layer.activation)\n",
    "        #print(\"Updated parameters of layer: {}\".format(self))\n",
    "        #parallel code\n",
    "        \n",
    "        #after parallel updates\n",
    "    \n",
    "    \n",
    "    def update_parameters_final_layer(self):\n",
    "        self.update_parameters_normal_layer()\n",
    "    \n",
    "    \n",
    "    def update_parameters(self):\n",
    "        if(self.final_layer):\n",
    "            self.update_parameters_final_layer()\n",
    "        else:\n",
    "            self.update_parameters_normal_layer()\n",
    "            \n",
    "        \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(1,2,3),(5,3,6)]\n",
    "y,z,a = zip(*x)\n",
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Layer(3, 6, False, True) # input_shape, max_num_neurons =10, input_layer= False, final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.48759683e-01,   2.43876500e-01,   1.40448632e-01,\n",
       "          6.81743337e-02,   2.66663494e-02,   9.41953690e-02],\n",
       "       [  2.47733650e-01,   2.38370060e-01,   5.16146411e-02,\n",
       "          8.36912951e-02,   1.92893212e-04,   1.50231531e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.feedforward(x).shape\n",
    "l1.del_a_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.53521814,  0.5782528 ,  0.16901455, -0.98745916, -0.99841179,\n",
       "         0.97268716],\n",
       "       [ 0.5476062 ,  0.6078422 ,  0.05459529, -0.97958471, -0.99999993,\n",
       "         0.90305873]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_activated = np.dot(x, l1.get_weights())+l1.get_bias()\n",
    "activated = sigmoid(non_activated[:,:3])\n",
    "activated2 = tanh(non_activated[:,3:])\n",
    "#np.stack([activated,activated2], axis =1)\n",
    "print(activated.shape)\n",
    "np.concatenate((activated,activated2),axis=1) #1st axis/column needs to be joined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.feedforward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.48759683e-01,   2.43876500e-01,   1.40448632e-01,\n",
       "          6.81743337e-02,   2.66663494e-02,   9.41953690e-02],\n",
       "       [  2.47733650e-01,   2.38370060e-01,   5.16146411e-02,\n",
       "          8.36912951e-02,   1.92893212e-04,   1.50231531e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.del_a_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 0.53521814],\n",
       "         [ 0.5476062 ]]), array([[ 0.14110621],\n",
       "         [ 0.19100337]]), array([[ 0.24875968],\n",
       "         [ 0.24773365]])), (array([[ 0.5782528],\n",
       "         [ 0.6078422]]), array([[ 0.31560506],\n",
       "         [ 0.438251  ]]), array([[ 0.2438765 ],\n",
       "         [ 0.23837006]])), (array([[ 0.16901455],\n",
       "         [ 0.05459529]]), array([[-1.59262748],\n",
       "         [-2.85166555]]), array([[ 0.14044863],\n",
       "         [ 0.05161464]])), (array([[ 0.07358979],\n",
       "         [ 0.09219036]]), array([[-2.53281082],\n",
       "         [-2.28717917]]), array([[ 0.06817433],\n",
       "         [ 0.0836913 ]])), (array([[ 0.0274181 ],\n",
       "         [ 0.00019293]]), array([[-3.56875085],\n",
       "         [-8.55298793]]), array([[ 0.02666635],\n",
       "         [ 0.00019289]])), (array([[ 0.89472095],\n",
       "         [ 0.81586147]]), array([[ 2.13989747],\n",
       "         [ 1.48855622]]), array([[ 0.09419537],\n",
       "         [ 0.15023153]]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[neuron_.forward_pass(x, weighted_as_well= True) for neuron_ in l1.neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_dict = {1:sigmoid, 2: tanh, 3: weighted_linear, 4: relu, 5: linear}\n",
    "class Artificial_Neural_Network:\n",
    "    def __init__(self, num_neurons_in_layer = [(5),(3,[(sigmoid, 2),(tanh, 1)]), (1,[(sigmoid, 1)])], cost_func = mse):\n",
    "        self.num_layers = len(num_neurons_in_layer)\n",
    "        self.num_neurons_in_each_layer = num_neurons_in_layer\n",
    "        self.input_dims = num_neurons_in_layer[0]\n",
    "        self.output_dims = num_neurons_in_layer[-1]\n",
    "        self.layers = []\n",
    "\n",
    "        self.cost_func = cost_func\n",
    "        self.layers.append(Layer(num_neurons_in_layer[0], num_neurons_in_layer[0],True)) #input layer\n",
    "        prv_layer_neurons = num_neurons_in_layer[0]\n",
    "        for i, layer_tuple in enumerate(num_neurons_in_layer[1:]):\n",
    "            if(not (i==len(num_neurons_in_layer)-2)): #since we started at two\n",
    "                self.layers.append(Layer(prv_layer_neurons, layer_tuple[0], False, False, layer_tuple[1] ))\n",
    "            else: #last layer/ output layer\n",
    "                self.layers.append(Layer(prv_layer_neurons, layer_tuple[0], False, True, layer_tuple[1] ))\n",
    "            self.layers[i].next_layer = self.layers[i+1] \n",
    "            self.layers[i+1].prev_layer = self.layers[i]\n",
    "            prv_layer_neurons = layer_tuple[0]\n",
    "            \n",
    "#def __init__(self, input_dims, max_num_neurons =10, input_layer= False, final_layer = False,\n",
    "#neuron_activation_tuples = [(sigmoid,10)] )\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        '''\n",
    "        Input of shape m*n_input_layer\n",
    "        Output of shape m*n_final_layer\n",
    "        '''\n",
    "        forward_val = inputs\n",
    "        for layer_ in self.layers:\n",
    "            layer_.set_previous_layer_activations(forward_val)\n",
    "            forward_val = layer_.feedforward(forward_val)\n",
    "            \n",
    "        return forward_val\n",
    "    \n",
    "    \n",
    "    def backprop(self, der_C_wrt_prediction):\n",
    "        '''Input of shape m*n final_layer\n",
    "           No outputs, only updates layer and neuron wise\n",
    "        '''\n",
    "        for layer in self.layers[::-1]:\n",
    "            if(layer.final_layer):\n",
    "                #print(der_C_wrt_prediction.shape)\n",
    "                #print(layer.del_a_z)\n",
    "                assert der_C_wrt_prediction.shape == layer.del_a_z.shape\n",
    "                del_C_wrt_unactivated = np.multiply(der_C_wrt_prediction, layer.del_a_z) #ELEMENT WISE\n",
    "                layer.set_layer_errors_wrt_primary_C(del_C_wrt_unactivated)\n",
    "                #print(\"DEL_C\")\n",
    "                #print(del_C_wrt_unactivated)\n",
    "                layer.update_parameters()\n",
    "                upper_layer_error = del_C_wrt_unactivated\n",
    "            \n",
    "            elif(layer.input_layer):\n",
    "                continue #DO nothing here\n",
    "            \n",
    "            else:\n",
    "                #del_C.... is of shape m*n and layer (the upper layer from which error is propogated) weights are of shape k*n \n",
    "                del_C_wrt_activated = np.dot(upper_layer_error, np.transpose(layer.next_layer.get_weights()))   #modify weights after backpropogating error to previous layer\n",
    "                del_C_wrt_unactivated = np.multiply(del_C_wrt_activated, layer.del_a_z)\n",
    "                #layer.del_a_z = del_C_wrt_unactivated #\n",
    "                #print(del_C_wrt_unactivated.shape)\n",
    "                layer.set_layer_errors_wrt_primary_C(del_C_wrt_unactivated) \n",
    "                layer.update_parameters() #update parameters by calculating del_C_wrt_parameter\n",
    "                #update layer weights\n",
    "                upper_layer_error = del_C_wrt_unactivated\n",
    "                \n",
    "            #del_C_wrt_unactivated =   #IMP for now a neuron's error is not equaivalent to layer's index error\n",
    "            #layer.set_layer_error() \n",
    "            \n",
    "        \n",
    "    def cost_calculation(self, y_true, y_pred):\n",
    "        return self.cost_func(y_true,y_pred)\n",
    "    \n",
    "    def training(self,x,y, print_metrics = True):\n",
    "        '''This takes in x and y and updates weights after processing entire input'''\n",
    "        '''For given x vector or matrix (M*N) where m is the number of tikraining examples, and y is the expected output (M*Y_dim):\n",
    "        1) Calculate forward pass (obtain resultant activations)\n",
    "        2) Obtain the associated cost with forward pass\n",
    "        3a) Adjust weights and biases depending on cost (CURRENTLY using gradient descent)\n",
    "        3b) Backpropogate error/cost through chain rule through each layer\n",
    "        3c) Define error as delC/del(prev_activation)--> use this to calculate delC/delW --> delC/del_prev_A* del_prev_A/delW\n",
    "        Update each weight and bias based on backpropogation\n",
    "        \n",
    "        Output of shape \n",
    "        \n",
    "        '''\n",
    "        if(x.ndim ==1): #only 1 training example provided (stochastic)\n",
    "            x = x.reshape(1, x.shape[0])\n",
    "            \n",
    "        #get forward pass predictions \n",
    "        forward_pass_output = self.feedforward(x)\n",
    "        #predictions = \n",
    "        predictions = forward_pass_output\n",
    "        \n",
    "        \n",
    "        #perform error calculation of relevant terms\n",
    "        assert y.shape == predictions.shape\n",
    "        avg_error, der_C_wrt_prediction = self.cost_calculation(y, predictions)\n",
    "        \n",
    "        if(print_metrics):\n",
    "            print(\"Predicted value: {} \".format(predictions))\n",
    "        \n",
    "        print(\"Average batch cost: {}\".format(avg_error))\n",
    "        #print(der_C_wrt_prediction.shape)\n",
    "        #perform parameter updates by calling relevant functions\n",
    "        self.backprop(der_C_wrt_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN = Artificial_Neural_Network([(3),(10,[(sigmoid, 5), (tanh, 5)]), (2,[(sigmoid, 1), (tanh, 1)]), (2,[(sigmoid, 2)])], cost_func= mse)\n",
    "ANN = Artificial_Neural_Network([(3),(50,[(sigmoid, 25),(tanh, 25)]), (2,[(linear, 2)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.array([[0.5,0.8],[0.8,0.5],[0.5,0.8],[0.8,0.5]])\n",
    "true_values2 = np.array([[5,8],[8,5],[5,8],[8,5]])\n",
    "xor_true_values = np.array([[1,0],[1,0],[0,1],[0,1]])\n",
    "input_values = np.array([[0.1, 0.1, 0.1],[0.9,0.9,0.9],[0.1, 0.1, 0.1],[0.9,0.9,0.9]])\n",
    "xor_input_values = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "#input_values, predicted_values = \n",
    "#ANN.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: [[ 1.41626931  1.25735186]] \n",
      "Average batch cost: 8.83395925489\n"
     ]
    }
   ],
   "source": [
    "ANN.training(np.array([[0.5,0.8,0.9]]), np.array([[5,6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear output \n",
    "for i in range(10000):\n",
    "    ANN.training(input_values, true_values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To do: \\nXOR classification example, how to classify for 2 examples, then extend to n examples'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non linear XOR problem and classification tasks \n",
    "\n",
    "''' To do: \n",
    "XOR classification example, how to classify for 2 examples, then extend to n examples'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.2: Questioning the constructed NNet\n",
    "\n",
    "## Relevant questions at this stage:\n",
    "\n",
    "1. What could be the possible consequences of using a high learning rate (>=0.8) in an mse cost error for linear output? Ans: This is done by exploring the derivation of backpropogation. \n",
    "2. What is the performance change when the the same value between 0 and 1 is predicted using a sigmoid output vs a linear output? (Performance = Time taken or number of computations to learn mapping\n",
    "3. If there are any differences in above question, how can we resolve this issue. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Q2\n",
    "## Exploring using sigmoidal output and linear output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear output activation\n",
    "ANN = Artificial_Neural_Network([(3),(50,[(sigmoid, 25),(tanh, 25)]), (2,[(linear, 2)])])\n",
    "for i in range(10000):\n",
    "    ANN.training(input_values, true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid output activation\n",
    "ANN = Artificial_Neural_Network([(3),(50,[(sigmoid, 25),(tanh, 25)]), (2,[(sigmoid, 2)])])\n",
    "for i in range(10000):\n",
    "    ANN.training(input_values, true_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Q3 \n",
    "\n",
    "#### Find the  culprits for slow learning in an >> ANN architecture<< (remember, we can create different architectures for better and faster learning. Thus, the question here is wrt an ANN architecture)\n",
    "\n",
    "#### Ans: How does learning occur--> Backprop--> When is learning 'slow' --> Which terms contribute to slow learning? ---> How can we elimintate the effect of these terms--> What are the changeable aspects in an ANN (remember -> forward calc, backward calc, and cost calculation) \n",
    "\n",
    "## Improving learning by changing cost function: Cross entropy derivation and implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##To do cross entropy example here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = Artificial_Neural_Network([(3),(50,[(sigmoid, 25),(tanh, 25)]), (2,[(linear, 2)])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1:  Applying the ANN for actual regression and classification tasks. Why are these tasks and datasets relevant for ML and AI? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our ANN for a regression task\n",
    "\n",
    "The regression dataset I have chosen is http://archive.ics.uci.edu/ml/datasets/yacht+hydrodynamics. How does deep learning help-> our network will take the provided inputs--> feedforward these to form representation out of the input. Each additional layer is stipulated to form a more complex/specific representation helpful in minimizing an objective- exs: classification accuracy, closer values to . (Bottom up processing) \n",
    "\n",
    "Traditionally, to make models for such tasks, researchers would extract useful features (through domain knowledge or some careful analysis/experimentation) and feed into a decision boundary model. \n",
    "In deep learning, the network ideally just requires the inputs and learns to extract the best features from the data. The final layer representation can then be used for classification/regression, etc. \n",
    "\n",
    "Q: Why is MNIST a useful dataset? Would a model performing 100% on MNIST be deemed a worthy number recongition system? Why or why not? (Remember to think about how we encounter numbers and how they look) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##regression dataset\n",
    "data_x = []\n",
    "data_y = []\n",
    "with open('./data/yacht_hydrodynamics.data') as f:\n",
    "   #read in the data \n",
    "    while(True):\n",
    "        line = f.readline()\n",
    "        if(line == '\\n'):\n",
    "            break\n",
    "        else:\n",
    "            line_list = line.split()\n",
    "            data_x.append(line_list[:-1])\n",
    "            data_y.append(line_list[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x = np.array(data_x, dtype=\"float\")\n",
    "data_y = np.array(data_y, dtype=\"float\").reshape(len(data_y),1)\n",
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN = Artificial_Neural_Network([(6),(200,[(sigmoid, 120),(tanh, 80)]), (1,[(linear, 1)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    ANN.training(data_x, data_y, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.2: Mechanisms of tuning for specific tasks and further improvement of the ANN learning algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, training a neural network for particular tasks seems pretty mystic. Further, while the architecture is mathematically sound, it does consume computational resources since there are so many variables to tune. \n",
    "Thus, there are some relevant questions to ask:\n",
    "1. How can training be made computationally less expensive and of higher performance?: \n",
    "    1. Introduce mathematical tricks: This includes changing and adding new elements to our nnet components such as a better cost function, constraining weights and activations through neurons.\n",
    "\n",
    "    2. Introduce different transformation architectures that still make mathematical sense. Ex: Convnets, RNN...\n",
    "\n",
    "    3. Further to point 2 , design architectures after undersanding the task requirements\n",
    "    \n",
    "    4. For computation, introduce a computation graph to compute gradients and variables. (This is only to limit \n",
    "\n",
    "2. Role of neural nets with respect to the question of AI:\n",
    "    1. Should we work on domain/problem-specific transformation algos/architectures and then see how to combine them for AI systems?  \n",
    "    2. Or instead, should we first clearly define the problems in AI as Minsky does in Perceptrons and Society of mind, and build algorithms suited to those problems and then work on adapting them for applications such as NLP, etc\n",
    "    3. Should we invest effort in introducing a theory of the mind- do psychological biases play a role in understanging, etc? \n",
    "\n",
    "3. How can training be made interpretable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.1: Deriving new architectures\n",
    "\n",
    "# RNN, addition of new 'variables' and how to optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3.2: What else can we use ANNs for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder and other unsupervised tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Neural network playground\n",
    "### 4.1) Implementing an Alzhiemers Neuron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron:\n",
    "    \n",
    "    def __init__(self, input_size,activation = sigmoid, lr= 0.1, belongs_to = None):\n",
    "        self.shape = (input_size,1)\n",
    "        self.weights = np.random.random(self.shape) - np.random.random(self.shape) #np.random.random(self.shape) #initialize random values  #np.zeros(self.shape)#\n",
    "        self.bias = np.array([0])\n",
    "        self.activation = activation\n",
    "        self.learning_rate = lr\n",
    "        self.surrounding_activations = None\n",
    "        self.belongs_to = belongs_to\n",
    "        self.neighbours = []\n",
    "        #self.previous_layer_activations = []\n",
    "        self.error_wrt_C = None\n",
    "        self.historical_activations = None #Hebbian learner to mantain co-occuring high activations together-> if n1 and n2 co-occur a lot, then if n1 occurs add a bias term?\n",
    "    def forward_pass(self,input_x, weighted_as_well = False): #weighted as well means just weighted, not activated\n",
    "        '''Input x is of shape m*n where m is batch size and n is input dims'''\n",
    "        weighted_input = np.dot(input_x, self.weights)+self.bias\n",
    "        if(weighted_as_well): #this returns both activated and non activated output \n",
    "            return (self.activation(weighted_input),weighted_input, self.activation(weighted_input, der = True)) \n",
    "        else:\n",
    "            return self.activation(weighted_input)\n",
    "    def adjust_parameters(self):\n",
    "        None\n",
    "    def calc_error(self,correct_y, predicted_y):\n",
    "        '''Calculates and returns error based on metric of neuron'''\n",
    "        return None \n",
    "    def calc_gradient(self, y,x):\n",
    "        '''Calculates gradient of y as function wrt x'''\n",
    "        if(self.activation == sigmoid):\n",
    "            return del_sigmoid()\n",
    "        elif(y== calc_error):\n",
    "            None\n",
    "        return y(x+1e-10)-y(x)/1e-10 #taking limit to calculate gradient \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "    def set_weights(self,weights):\n",
    "        self.weights = weights\n",
    "    def set_bias(self, bias):\n",
    "        self.bias  = bias\n",
    "    \n",
    "    def set_error_wrt_C(self, error_wrt_C):\n",
    "        sefl.error_wrt_C = error_wrt_C\n",
    "    def give_relevant_info(self, info): #this function serves to receive relevant information, add any variables here\n",
    "        None\n",
    "    def add_neighbour(self,neigbhour):\n",
    "        self.neighbours.append(neigbhour)\n",
    "        \n",
    "    #def set_previous_layer_activations(self, previous_layer_activations):\n",
    "     #   self.previous_layer_activations = previous_layer_activations\n",
    "        \n",
    "    #def gradient_descent(self, )\n",
    "    def update_parameters(self, previous_layer_activations):\n",
    "        '''Previous layer activations  is of shape m*n\n",
    "           Error_wrt_c is of shape m*1\n",
    "        '''\n",
    "        \n",
    "        '''CURRENTLY a single error will be sent across input parameters (weights and bias)'''\n",
    "        #1. BACKPROPOGATION TO UPDATE WEIGHTS AND BIAS WRT OVERALL COST \n",
    "        #assert(self.error_wrt_C)\n",
    "       # print(\"del_C_wrt_del_z\",self.error_wrt_C)\n",
    "        avg_neuron_error = np.mean(self.error_wrt_C, axis =0)\n",
    "        num_examples = self.error_wrt_C.shape[0]\n",
    "        #weight_der = input_error*a_prev\n",
    "        #bias_der = input_error*1\n",
    "        bias_der = avg_neuron_error*1\n",
    "        self.bias = self.bias - self.learning_rate*self.bias\n",
    "        \n",
    "        weight_ders = np.dot(np.transpose(previous_layer_activations), self.error_wrt_C)\n",
    "        avg_weight_ders = weight_ders/num_examples\n",
    "        self.weights = self.weights - self.learning_rate*avg_weight_ders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alzheimer_neuron(neuron):\n",
    "    '''This is just a fun example of a neuron\n",
    "    It does not represent the full process of what happens during development of Alzheimers disease\n",
    "    But a friend of mine gave the following gist: Continous high activations can fire the neuron off permanently\n",
    "    Below neurons get deactivated for a given time if having too many high activations\n",
    "    How can this be useful: Continously high activated neurons can lead to high gradient values, and learning might be forced through other patterns if neuron is being excited by everything'''\n",
    "    def __init__(self, input_size, starting_state = 1, deactivation_threshold = 0.8, deactivation_time = 5, history = 10, num_continous_exceed = 5, max_high_deactivations = 50, activation = sigmoid, lr= 0.1, belongs_to = None):\n",
    "        super(alzheimer_neuron, self).__init__(activation, lr, belongs_to)\n",
    "        self.state = starting_state #1 means active, 0 means deactive\n",
    "        self.previous_high_activations = []\n",
    "        self.deactivation_threshold = deactivation_threshold #activation value above which \n",
    "        self.deactivation_time = deactivation_time #number of steps for deactivation\n",
    "        self.max_high_deactivations = 50 #after 50 above threshold deactivations, neuron is deactivated for steps = deactivation time\n",
    "        self.max_num_continously_exceeded = num_continous_exceed\n",
    "        self.continously_exceeeded = 0 #how many in continous have exceeded\n",
    "        self.history = history #NOT USed now if previous activations have to be limited\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Implementing associtiavity in neurons: What is the new variable, how do we tune the relevant parameter, what's the cost func? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section -1: To dos and some more Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IMP TO DO\n",
    "1)  def update_parameters_normal_layer(self): Line 140 of layer class \n",
    "        #Parallelize and obtain error axis through a getter function\n",
    "        \n",
    "        for i, n_ in enumerate(self.neurons):\n",
    "            #WE NEED A FUNCTION TO REPLACE THIS- obtain batchwise errors/axis\n",
    "            n_.error_wrt_C = self.errors_wrt_primary_C[:,i] \n",
    "            n_.update_parameters(self.prev_layer.activation)\n",
    "        #print(\"Updated parameters of layer: {}\".format(self))\n",
    "        \n",
    "        '''\n",
    "\n",
    "'''Fun stuff to do\n",
    "1) Alzheimer neuron-> as mentioned by Bobby\n",
    "2) Genetic neurons --> Each neuron is represented by some gene which makes it behave a particular way. Question is: Should genes be precoded or genes will be learned as training progresses. Similar gene neurons behave in simlar ways \n",
    "4) What is a possible optimum/cost for judging how well a concept is learned by a machine? \n",
    "3) Is a cost function the best way to judge a nnets performance? What is the best way to determine the best configuration of weights and errors. Is it wrong to learn through gradients? \n",
    "'''\n",
    "\n",
    "'''Stuff to do:\n",
    "VI: <B> ADD CROSS entropy </B>\n",
    "1) Parrallelize update parameters for layer class\n",
    "2) Generalize backpropogate so that axis for each --> batch, neurons, time etc can be represented\n",
    "3) Autoencoder\n",
    "4) RNN\n",
    "5) \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''To do:\n",
    "1) Insert hebbian learner--> calculate associative strength of neurons, if two neurons co-occur then make a weight between the two\n",
    "2) Insert a redundant learner/ Alzheimer's neuron --> mantain a self.last_10activations list, see rate of change of input-> last_10_inputs, EXTEND the base neuron class \n",
    "5) VVVVIP: Parallelize the update parameters in Layer class for each neuron  \n",
    "3) ADD a variable in code to accept axis of error and axis of other things \n",
    "4) Generalize the backpropogate function in layer to specify axis of error\n",
    "LINE 140 Parallelize it\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
